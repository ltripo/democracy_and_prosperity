{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set()\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "def full_display(self):\n",
    "    with pd.option_context('display.max_rows',None,'display.max_columns',None):\n",
    "        display(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please check my study's report to learn more about what I'm doing here. in fact, if you don't, this will hardly make any \n",
    "# sense. you can retrieve it from https://bit.ly/2m6vRE8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll start by loading the rsf dataset\n",
    "\n",
    "# unfortunarely, I had to get rid of the arabic and farsi characters first, otherwise pandas won't parse the file \n",
    "# I'm also specifying comma as a decimal separator as well as converting data from the 'Score 2019' column to float\n",
    "\n",
    "url='https://raw.githubusercontent.com/ltripo/democracy_and_prosperity/master/datasets/A.1.I_rsf_raw_dataset.csv'\n",
    "\n",
    "df_rsf=pd.read_csv(url,decimal=',',dtype={'Score 2019':np.float})\n",
    "\n",
    "full_display(df_rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all I am interested in are country names and the latest (ie. 2019) score for each of them, which is why I'm' dropping all\n",
    "# columns except those two\n",
    "\n",
    "# on an afterthought though, I'll keep the 'zone' column, this will help handle missing values in what comes next\n",
    "\n",
    "df_rsf=df_rsf[['EN_country','Score 2019','Zone']]\n",
    "\n",
    "# I'll rename the remaining columns\n",
    "\n",
    "df_rsf=df_rsf.rename(columns={'EN_country':'country_name','Score 2019':'rsf_index','Zone':'region'})\n",
    "\n",
    "# using capitalised country names is nevertheless a bit clumsy, so I'm converting them to lower case. by the way I'm doing the\n",
    "# same to zone names, all in one loop\n",
    "\n",
    "for col in df_rsf.columns:\n",
    "    if df_rsf[col].dtype=='object':\n",
    "        df_rsf[col]=df_rsf[col].str.lower()\n",
    "\n",
    "# plus I want rsf_index to go down, instead of up, from more freedom of speech to less, so I'm reckoning its reciprocal\n",
    "\n",
    "df_rsf['rsf_index']=df_rsf['rsf_index']**(-1)\n",
    "    \n",
    "full_display(df_rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll now scale the data to some arbitrary range (my personal pick is 5-10) by using minmaxscaler\n",
    "\n",
    "mms=MinMaxScaler(feature_range=(5,10))\n",
    "\n",
    "df_rsf[['rsf_index']]=mms.fit_transform(df_rsf[['rsf_index']])\n",
    "\n",
    "full_display(df_rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all that's left to do now is replace 'russian federation' with 'russia', so that we don't run into inconsistencies \n",
    "# later on\n",
    "\n",
    "df_rsf.replace(to_replace='russian federation',value='russia',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm done with this dataset for now, let's move on to the next one\n",
    "\n",
    "url='https://raw.githubusercontent.com/ltripo/democracy_and_prosperity/master/datasets/A.1.II_pew_raw_dataset.csv'\n",
    "\n",
    "df_pew=pd.read_csv(url)\n",
    "\n",
    "df_pew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of the way I've pulled the data from the original source (which was a PDF file) all numeric columns are\n",
    "# overfilled, so I need to clean them. only the first two digits in each cell matter, and the resulting data \n",
    "# type should be float\n",
    "\n",
    "for col in df_pew.columns[1:]:\n",
    "    df_pew[col]=df_pew[col].str[:2]\n",
    "    df_pew=df_pew.astype({col: 'float'})\n",
    "\n",
    "df_pew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I now have to deal with the NaN at line 19. taking into account what Pew say in their documentation,\n",
    "# I believe the best alternative is to impute that NaN with its column's lowest value\n",
    "\n",
    "df_pew['explicit'].fillna(df_pew['explicit'].min(),inplace=True)\n",
    "\n",
    "df_pew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it so happens that I have no interest in so many columns, I want just the one. so I'm reckoning the overall geometric mean \n",
    "# (as in the present situation, factors contribute towards a final result) and contracting columns so as to end up with the\n",
    "# essential information only\n",
    "\n",
    "# additionally, I'm converting country names to lower case\n",
    "\n",
    "df_pew['pew_index']=(df_pew['policies']*df_pew['beliefs']*df_pew['protests_v']*df_pew['explicit']*df_pew['protests_e']*df_pew['destabilise']*df_pew['security'])**(1/7)\n",
    "\n",
    "df_pew['country_name']=df_pew['country_name'].str.lower()\n",
    "\n",
    "df_pew=df_pew[['country_name','pew_index']]\n",
    "\n",
    "# I would also like to scale the data and order the rows by score from highest to lowest so I can take a decent look at the\n",
    "# dataframe\n",
    "\n",
    "df_pew[['pew_index']]=mms.fit_transform(df_pew[['pew_index']])\n",
    "\n",
    "df_pew.sort_values('pew_index',ascending=False,inplace=True)\n",
    "\n",
    "df_pew=df_pew.reset_index(drop=True)\n",
    "\n",
    "df_pew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there it is. now I need to assign each country its geographical region so I can successfully merge this dataframe with the  \n",
    "# previous one and optimise the treatment of missing values. I will go on and create a new dataframe already, for this will\n",
    "# make things easier\n",
    "\n",
    "df_rsf_pew=pd.merge(df_rsf,df_pew,how='left',on='country_name',sort=True)\n",
    "\n",
    "df_rsf_pew=df_rsf_pew[['country_name','region','rsf_index','pew_index']]\n",
    "\n",
    "full_display(df_rsf_pew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I'm not sure all these region names are easy to deal with, so I'm renaming them to current usage (in english)\n",
    "\n",
    "df_rsf_pew['region']=df_rsf_pew['region'].map({'afrique':'africa','asie-pacifique':'asia_pacific','eeac':'east_asia','mena':'middle_east','ue balkans':'europe','north_am':'north_am','latam_carib':'latam_carib'})\n",
    "\n",
    "full_display(df_rsf_pew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am now left with a lot of missing values to handle. fortunately, I already have countries sorted out by region so  \n",
    "# I can simply impute to each country marked with a NaN in pew_index the region average for that index\n",
    "\n",
    "def region_average(self,chosen_index):\n",
    "    for r in self['region'].unique():\n",
    "        region_average=np.mean(self[chosen_index][self['region']==r])\n",
    "        for i in range(len(self.index)):\n",
    "            if (self['region'][i]==r and np.isnan(self[chosen_index][i])==True):\n",
    "                self[chosen_index][i]=region_average\n",
    "\n",
    "region_average(df_rsf_pew,'pew_index')\n",
    "        \n",
    "full_display(df_rsf_pew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's time to face the truth. I've picked both these indices (rsf and pew) in the hope that each tells a different\n",
    "# story, ie. that they are not significantly correlated, otherwise I'd would be inflating variance. let's see if\n",
    "# data science proves me wrong\n",
    "\n",
    "# I'm using spearman because it's more appropriate for rank-based comparisons\n",
    "\n",
    "df_rsf_pew.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables aren't uncorrelated, and maybe in a different context one of them could be dropped, but it's still nothing\n",
    "# scary for the purposes of our study. they still tell a slightly different story each. therefore, I will keep both onboard for\n",
    "# the time being\n",
    "\n",
    "# I'm now moving on to a different kind of information, viz. that on social mobility but remember, I'm still working on\n",
    "# the democracy side \n",
    "\n",
    "url='https://raw.githubusercontent.com/ltripo/democracy_and_prosperity/master/datasets/A.2.I_oecd_migration_raw_dataset.csv'\n",
    "\n",
    "df_oecd_migration_1=pd.read_csv(url)\n",
    "\n",
    "full_display(df_oecd_migration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've tried to make the above more usable with the spreadsheet editor, but it still looks terrible. I only need every other\n",
    "# row as well as the third column's values, so here's what I'll do\n",
    "\n",
    "df_oecd_migration=df_oecd_migration_1.copy()\n",
    "\n",
    "x=np.arange(0,74)\n",
    "y=np.arange(0,74,2)\n",
    "\n",
    "for i,j in zip(x,y):\n",
    "    for col in df_oecd_migration_1.columns:\n",
    "        df_oecd_migration[col][i]=df_oecd_migration_1[col][j]\n",
    "\n",
    "df_oecd_migration=df_oecd_migration.drop(df_oecd_migration.index[[range(37,74)]])\n",
    "\n",
    "del df_oecd_migration['percentage']\n",
    "\n",
    "df_oecd_migration=df_oecd_migration.rename(columns={'% of total population':'percentage'})        \n",
    "\n",
    "df_oecd_migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks pretty clean right now; we still have a few missing values, though. now, there's no point averaging them out here,\n",
    "# let me worry about that later. we can simply drop those rows for now\n",
    "\n",
    "df_oecd_migration=df_oecd_migration.drop(df_oecd_migration[df_oecd_migration['percentage']=='..'].index,axis=0)\n",
    "\n",
    "df_oecd_migration=df_oecd_migration.reset_index(drop=True)\n",
    "        \n",
    "df_oecd_migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there you go. now let's move on to the next dataset\n",
    "\n",
    "url='https://raw.githubusercontent.com/ltripo/democracy_and_prosperity/master/datasets/A.2.II_oecd_generations_raw_dataset.csv'\n",
    "\n",
    "df_oecd_generations=pd.read_csv(url,dtype={'generations':np.float})\n",
    "\n",
    "df_oecd_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no major issues here since I had to type this one from scratch. next one on the list is the world bank's ease of\n",
    "# doing business score\n",
    "\n",
    "url='https://raw.githubusercontent.com/ltripo/democracy_and_prosperity/master/datasets/A.2.III_ease_db_world_bank_raw_dataset.csv'\n",
    "\n",
    "df_wb_ease=pd.read_csv(url)\n",
    "\n",
    "full_display(df_wb_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm converting country names to lower case\n",
    "\n",
    "df_oecd_migration=df_oecd_migration.reset_index(drop=True)\n",
    "\n",
    "df_oecd_migration['country_name']=df_oecd_migration['country_name'].str.lower()\n",
    "df_wb_ease['country_name']=df_wb_ease['country_name'].str.lower()\n",
    "    \n",
    "# next thing I'm doing is, I'm sorting these dataframes' rows in ascending order, then scaling them\n",
    "\n",
    "# let's see: a high percentage of migrants is good, a great ease of doing business is also good. so I just have to apply\n",
    "# inverse to the df_oecd_generations dataframe's 'generations' colum\n",
    "\n",
    "df_oecd_generations['generations']=df_oecd_generations['generations']**(-1)\n",
    "\n",
    "df_oecd_migration[['percentage']]=mms.fit_transform(df_oecd_migration[['percentage']])\n",
    "df_oecd_generations[['generations']]=mms.fit_transform(df_oecd_generations[['generations']])\n",
    "df_wb_ease[['ease_db']]=mms.fit_transform(df_wb_ease[['ease_db']])\n",
    "\n",
    "df_oecd_migration=df_oecd_migration.rename(columns={'percentage':'oecd_mig_index'})\n",
    "df_oecd_generations=df_oecd_generations.rename(columns={'generations':'oecd_gen_index'})\n",
    "df_wb_ease=df_wb_ease.rename(columns={'ease_db':'wb_index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll now do pretty much the same as we did before, by merging data and treating missing values. by performing a full \n",
    "# outer merge, we'll preserve all countries from all datasets and therefore, avoid losing information\n",
    "\n",
    "df_democracy=pd.merge(df_wb_ease,df_rsf_pew,how='outer',on='country_name',sort=True)\n",
    "df_democracy=pd.merge(df_democracy,df_oecd_migration,how='outer',on='country_name',sort=True)\n",
    "df_democracy=pd.merge(df_democracy,df_oecd_generations,how='outer',on='country_name',sort=True)\n",
    "\n",
    "df_democracy=df_democracy[['country_name','region','rsf_index','pew_index','oecd_mig_index','oecd_gen_index','wb_index']]\n",
    "\n",
    "full_display(df_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's never too late to search for inconsistencies\n",
    "\n",
    "isna_rsf=df_democracy['rsf_index'].isnull()==True\n",
    "isna_wb=df_democracy['wb_index'].isnull()==True\n",
    "\n",
    "df_democracy[isna_rsf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_democracy[isna_wb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we seem to be quite alright, except for the oecs which is not really a country, which is why I'm dropping it\n",
    "\n",
    "df_democracy=df_democracy.drop(130).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have a lot of missing values left to work on though\n",
    "\n",
    "df_democracy.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows with at least one missing value\n",
    "\n",
    "df_democracy.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's worry about the regions column\n",
    "\n",
    "isna_region=df_democracy['region'].isnull()==True\n",
    "\n",
    "df_democracy[isna_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there seem to be no major issues here. I'll just jump in and set the region manually\n",
    "\n",
    "df_democracy.at[5,'region']='latam_carib'\n",
    "df_democracy.at[11,'region']='latam_carib'\n",
    "df_democracy.at[14,'region']='latam_carib'\n",
    "df_democracy.at[46,'region']='latam_carib'\n",
    "df_democracy.at[67,'region']='asia_pacific'\n",
    "df_democracy.at[90,'region']='asia_pacific'\n",
    "df_democracy.at[109,'region']='asia_pacific'\n",
    "df_democracy.at[113,'region']='asia_pacific'\n",
    "df_democracy.at[132,'region']='asia_pacific'\n",
    "df_democracy.at[140,'region']='north_am'\n",
    "df_democracy.at[146,'region']='europe'\n",
    "df_democracy.at[155,'region']='asia_pacific'\n",
    "df_democracy.at[162,'region']='latam_carib'\n",
    "df_democracy.at[163,'region']='latam_carib'\n",
    "df_democracy.at[164,'region']='latam_carib'\n",
    "df_democracy.at[170,'region']='latam_carib'\n",
    "df_democracy.at[189,'region']='asia_pacific'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it shouldn't be a problem now to fill in missing values with regional averages\n",
    "\n",
    "region_average(df_democracy,'rsf_index')\n",
    "region_average(df_democracy,'pew_index')\n",
    "region_average(df_democracy,'oecd_gen_index')\n",
    "region_average(df_democracy,'wb_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as for the oecd_mig_index: apparently, none of the countries with missing values seem to be particularly sought\n",
    "# after by international migrants, which is probably why they were not surveyed by the oecd. it is therefore appropriate to \n",
    "# impute a value of 5 to each of them (remember, we are working on a normalised 5-10 scale)\n",
    "\n",
    "df_democracy['oecd_mig_index'][df_democracy['oecd_mig_index'].isnull()]=5\n",
    "df_democracy['oecd_mig_index']=pd.to_numeric(df_democracy['oecd_mig_index'])\n",
    "        \n",
    "full_display(df_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as to the oecd_gen_index: there are no averages for either east_asia or middle_east countries. by peeking into these\n",
    "# countries' gini coefficients, it appears that we're rather safe imputing to them the asia_pacific average \n",
    "\n",
    "df_democracy['oecd_gen_index'][df_democracy['oecd_gen_index'].isnull()]=np.mean(df_democracy['oecd_gen_index'][df_democracy['region']=='asia_pacific']) \n",
    "\n",
    "full_display(df_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we move on to the next step, I seem to recall that some countries from the df_oecd_migration dataset had no data\n",
    "# associated with them. this doesn't necessarily mean that they are not sought after by international migrants. it's hard to\n",
    "# infer the data, though, from other datasets, even from the same survey, but it's certainly possible to infer which\n",
    "# ones are, and which are not, sought after by international migrants and/or asylum seekers based on the respective inflows.\n",
    "# so I'll go ahead and attribute these countries mean overall, instead of minimum, value to avoid their getting penalised\n",
    "\n",
    "oecd_mig_average=np.mean(df_democracy['oecd_mig_index'])\n",
    "\n",
    "l_countries=['canada','czech republic','ireland','israel','japan','south korea','mexico','new zealand','russia']\n",
    "\n",
    "for c in l_countries:\n",
    "    df_democracy['oecd_mig_index'][df_democracy['country_name']==c]=oecd_mig_average\n",
    "\n",
    "df_democracy[df_democracy['oecd_mig_index']==oecd_mig_average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now we're getting a bit more scientific. let's compare the above variables' variance to see if there's some one we \n",
    "# can drop\n",
    "\n",
    "l_labels=['rsf_index','pew_index','oecd_mig_index','oecd_gen_index','wb_index']\n",
    "\n",
    "l_variances=[]\n",
    "\n",
    "for var in l_labels:\n",
    "    l_variances.append(round(df_democracy[var].var(),2))\n",
    "    print(f'{var}: {round(df_democracy[var].var(),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or graphically\n",
    "\n",
    "def bar_plot_var_1():\n",
    "    sns.set()\n",
    "    index=np.arange(len(l_labels))\n",
    "    plt.bar(index,l_variances,color=('b'))\n",
    "    plt.ylabel('variance',fontsize=12)\n",
    "    plt.xticks(index,l_labels,fontsize=11,rotation=30)\n",
    "    plt.title('democracy variables as compared by variance',fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "bar_plot_var_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at how they correlate with one another\n",
    "\n",
    "df_democracy.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or graphically\n",
    "\n",
    "def heat_plot_var(df):\n",
    "    fig,ax=plt.subplots(figsize=(5,5))\n",
    "    mask=np.zeros_like(df.corr(method='spearman').abs())\n",
    "    mask[np.triu_indices_from(mask)]=1\n",
    "    sns.heatmap(df.corr(method='spearman').abs(),mask= mask,ax=ax,cmap='coolwarm',annot=True)\n",
    "    \n",
    "heat_plot_var(df_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by looking at the above graphs, I don't like the way pew_index shows a lot of variance and rather minor covariance, \n",
    "# since it was built on regional averages, so it's not supposed to be a quality index. besides that, the oecd_mig_index has\n",
    "# rather poor relative variance. also, I'm now afraid the oecd_gen_index does not really tell a different story from the \n",
    "# wb_index and if that checks out, we would be inflating variance and reinforcing trends, which is obviously against good\n",
    "# practice. therefore, I am dropping these indices, based on a more informed analysis which I'm now able to perform\n",
    "\n",
    "# for the purposes of the machine learning part I'll prefer to calculate a democratic index as a combination  \n",
    "# of its parts. for the lack of a solid basis to attribute weights, this calculation is going to be a simple\n",
    "# geometric mean. I'm using the geometric mean because in the present situation, factors contribute towards a final result\n",
    "\n",
    "# before we proceed, I have decided to recast the wb_index as its natural logarithm, as it is more likely than not,\n",
    "# in my opinion, that the ease of doing business makes a lot of difference at the bottom, yet has only marginal utility \n",
    "# at the top. this may not make a great difference but still makes sense to me\n",
    "\n",
    "mms_wb=MinMaxScaler(feature_range=(1,np.exp(1)))\n",
    "\n",
    "df_democracy[['wb_index']]=mms_wb.fit_transform(df_democracy[['wb_index']])\n",
    "df_democracy['wb_index']=np.log(df_democracy['wb_index'])\n",
    "df_democracy[['wb_index']]=mms.fit_transform(df_democracy[['wb_index']])\n",
    "\n",
    "df_democracy['democracy_index']=((df_democracy['rsf_index'])*(df_democracy['wb_index']))**(1/2)\n",
    "df_democracy[['democracy_index']]=mms.fit_transform(df_democracy[['democracy_index']])\n",
    "    \n",
    "df_democracy=df_democracy.drop(columns=['pew_index','oecd_mig_index','oecd_gen_index'],axis=1)\n",
    "\n",
    "df_democracy['country_name']=df_democracy['country_name'].str.rstrip()\n",
    "df_democracy.sort_values('country_name',inplace=True)\n",
    "\n",
    "df_democracy=df_democracy.reset_index(drop=True)\n",
    "\n",
    "full_display(df_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is still one thing left to do before we move on to the prosperity prong, which is to study both the variance, and\n",
    "# therefore relevance, of the democracy_index variable as well as its covariance with the previous indices\n",
    "\n",
    "l_labels=['rsf_index','wb_index','democracy_index']\n",
    "\n",
    "l_variances=[]\n",
    "\n",
    "for var in l_labels:\n",
    "    print(f'{var}: {round(df_democracy[var].var(),2)}')\n",
    "    l_variances.append(round(df_democracy[var].var(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or graphically\n",
    "\n",
    "def bar_plot_var_2():\n",
    "    sns.set()\n",
    "    index=np.arange(len(l_labels))\n",
    "    plt.bar(index,l_variances,color=['b','b','r'])\n",
    "    plt.ylabel('variance',fontsize=12)\n",
    "    plt.xticks(index,l_labels,fontsize=11,rotation=30)\n",
    "    plt.title('democracy variables as compared by variance',fontsize=12)\n",
    "    plt.hlines(round(df_democracy['democracy_index'].var(),2),0,len(l_labels)-1,linestyle='dashed',color='red')\n",
    "    plt.show()\n",
    "    \n",
    "bar_plot_var_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is still significant variance associated with democracy_index, though it has shrinked which I assume is due to a\n",
    "# preexisting correlation between rsf_index and wb_index. but what about covariance? let's look it over graphically\n",
    " \n",
    "heat_plot_var(df_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that looks all right to me, insofar as democracy_index was calculated on the basis of the other two, yet probably\n",
    "# no single one of them could simply replace it. we are now ready to move on to prosperity\n",
    "# as you can see in my report file, I have decided to use the world happiness ranking alone for that\n",
    "\n",
    "url='https://raw.githubusercontent.com/ltripo/democracy_and_prosperity/master/datasets/B.I_gwp_raw_dataset.csv'\n",
    "\n",
    "df_gwp=pd.read_csv(url)\n",
    "\n",
    "full_display(df_gwp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, I have to sort this mess out thereby getting a clean dataframe\n",
    "# using regular expressions is best\n",
    "\n",
    "clean_cn=df_gwp['country_name'].str.extract(r'([^0-9\\.()]+)')\n",
    "clean_ind=df_gwp['country_name'].str.extract(r'(\\d.\\d\\d\\d)')\n",
    "\n",
    "df_gwp['country_name']=clean_cn\n",
    "df_gwp['gwp_index']=clean_ind\n",
    "\n",
    "df_gwp['gwp_index']=pd.to_numeric(df_gwp['gwp_index'])\n",
    "\n",
    "# I need to set country names to lower case, and also scale gwp_index\n",
    "\n",
    "df_gwp['country_name']=df_gwp['country_name'].str.lower()\n",
    "\n",
    "df_gwp[['gwp_index']]=mms.fit_transform(df_gwp[['gwp_index']])\n",
    "    \n",
    "df_gwp.sort_values('country_name',inplace=True)\n",
    "\n",
    "df_gwp=df_gwp.reset_index(drop=True)\n",
    "\n",
    "full_display(df_gwp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there you go. it's now time to merge the df_democracy and df_gwp dataframes into a df_dem_prosp consolidated dataframe\n",
    "\n",
    "df_democracy['country_name']=df_democracy['country_name'].str.strip()\n",
    "\n",
    "df_gwp['country_name']=df_gwp['country_name'].str.strip()\n",
    "\n",
    "df_dem_prosp=pd.merge(df_democracy,df_gwp,how='outer',on='country_name',sort=True)\n",
    "df_dem_prosp=df_dem_prosp[['country_name','region','democracy_index','gwp_index']]\n",
    "df_dem_prosp=df_dem_prosp.rename(columns={'gwp_index':'prosperity_index'})\n",
    "\n",
    "full_display(df_dem_prosp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last missing values to impute\n",
    "\n",
    "region_average(df_dem_prosp,'prosperity_index')\n",
    "\n",
    "full_display(df_dem_prosp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look now at what we've got. this is our final dataset as plotted on a scatter chart\n",
    "\n",
    "def scat_plot(df):\n",
    "    sns.set()\n",
    "    plt.scatter(df[:,0],df[:,1],c='b',s=10)\n",
    "    plt.title('Democracy versus Prosperity for 196 Countries')\n",
    "    plt.xlabel('Democracy')\n",
    "    plt.ylabel('Prosperity')\n",
    "    plt.show()\n",
    "    \n",
    "X=df_dem_prosp.as_matrix(columns=df_dem_prosp.columns[2:])\n",
    "labels_true=df_dem_prosp['country_name']\n",
    "\n",
    "scat_plot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please refer to my study's report in order to understand why I chose affinity propagation and learn more about how it works.\n",
    "# the use of a machine learning algorithm involves trade-offs. in this case, a number of clusters which is either too\n",
    "# narrow or too large has poor explanatory value. explaining relationships between countries requires assigning \n",
    "# meaningful characteristics to country clusters, but one can only do that if those clusters appear to make \n",
    "# sense when visualised. you can toy with the algorithm's parameters yourself and come to your own conclusions. when I took\n",
    "# my turn I kept in mind that I needed to keep the silhouette score at an acceptable level (which I \n",
    "# made sure I did by gauging the damping and preference parameters)\n",
    "\n",
    "# the snippet below was adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-\n",
    "# affinity-propagation-py\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "# #############################################################################\n",
    "# Compute Affinity Propagation\n",
    "af = AffinityPropagation(damping=.8,preference=-5,verbose=True).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Score: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set()\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    \n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title('Democracy versus Prosperity for 196 Countries')\n",
    "plt.xlabel('Democracy')\n",
    "plt.ylabel('Prosperity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sake of sound methodology, I will now perform a simple demonstration of how both k-means and dbscan performed \n",
    "# poorly on this dataset\n",
    "\n",
    "# the snippet below was adapted from:\n",
    "# https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def elbow(X):\n",
    "    distortions = [] \n",
    "    inertias = [] \n",
    "    mapping1 = {} \n",
    "    mapping2 = {} \n",
    "    K = range(1,10)    \n",
    "    for k in K:\n",
    "        #Building and fitting the model \n",
    "        kmeanModel = KMeans(n_clusters=k).fit(X) \n",
    "        kmeanModel.fit(X)\n",
    "        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'),axis=1)) / X.shape[0]) \n",
    "        inertias.append(kmeanModel.inertia_)\n",
    "        mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'),axis=1)) / X.shape[0] \n",
    "        mapping2[k] = kmeanModel.inertia_ \n",
    "\n",
    "    # Using the different values of Distortion\n",
    "    print('Distortion:')\n",
    "    for key,val in mapping1.items():\n",
    "        print(str(key)+' : '+str(val))\n",
    "\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('Values of K')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method using Distortion')\n",
    "    plt.show() \n",
    "\n",
    "    # Using the different values of Inertia\n",
    "    print('Inertia:')\n",
    "    for key,val in mapping2.items():\n",
    "        print(str(key)+' : '+str(val))\n",
    "\n",
    "    plt.plot(K, inertias, 'bx-')\n",
    "    plt.xlabel('Values of K')\n",
    "    plt.ylabel('Inertia') \n",
    "    plt.title('The Elbow Method using Inertia')\n",
    "    plt.show()\n",
    "    \n",
    "elbow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both above elbow graphs point to 4 as the optimal number of clusters. let's see how the silhouette score turns out  \n",
    "# for both 4 and 8 clusters\n",
    "\n",
    "range_n_clusters=[4,8]\n",
    "\n",
    "for n_cluster in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_cluster)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.predict(X)\n",
    "    print (n_cluster, metrics.silhouette_score(X,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now dbscan\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see these algorithms did not yield a convenient partition of the dataset (if at all)\n",
    "\n",
    "# we are now ready to export the final data for further analysis. but first we need to create a new column in the \n",
    "# dataframe to accomodate cluster number information\n",
    "\n",
    "df_dem_prosp['cluster_#']=AffinityPropagation(damping=.8,preference=-5).fit_predict(X)\n",
    "\n",
    "df_dem_prosp.to_csv('C:/foo/bar/export_dem_prosp.csv',index=False)\n",
    "\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
